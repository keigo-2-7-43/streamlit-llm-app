import streamlit as st
from dotenv import load_dotenv
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
api_key = os.getenv("OPENAI_API_KEY")

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ§  å°‚é–€å®¶LLMã‚¢ãƒ—ãƒª")
st.write("ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€è³ªå•ã«å¯¾ã—ã¦é¸æŠã—ãŸå°‚é–€å®¶ã«ãªã‚Šãã£ãŸLLMãŒå›ç­”ã—ã¾ã™ã€‚")
st.write("ä»¥ä¸‹ã‹ã‚‰å°‚é–€å®¶ã‚’é¸ã³ã€è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# å°‚é–€å®¶ã®é¸æŠè‚¢
expert_options = {
    "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶": "ã‚ãªãŸã¯ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èª²é¡Œã«å¯¾ã—ã¦å®Ÿè·µçš„ã‹ã¤å‰µé€ çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "æ „é¤Šå­¦ã®å°‚é–€å®¶": "ã‚ãªãŸã¯æ „é¤Šå­¦ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥åº·ã«é–¢ã™ã‚‹ç–‘å•ã«å¯¾ã—ã¦ã€ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸå›ç­”ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
}
expert_choice = st.radio("å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š", list(expert_options.keys()))

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
user_input = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

# å›ç­”ç”Ÿæˆé–¢æ•°
def get_response_from_llm(question: str, expert_type: str) -> str:
    system_message = expert_options[expert_type]

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(openai_api_key=api_key, model="gpt-3.5-turbo")
    chain = prompt | llm
    return chain.invoke({"input": question})

# å®Ÿè¡Œå‡¦ç†
if user_input:
    with st.spinner("è€ƒãˆä¸­..."):
        response = get_response_from_llm(user_input, expert_choice)
        st.success("å›ç­”ï¼š")
        st.write(response.content)
